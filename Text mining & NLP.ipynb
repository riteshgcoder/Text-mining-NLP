{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus #corpus refers to a collection of texts. Such collections may be formed of a single language of texts, or can span multiple languages -- there are numerous reasons for which multilingual corpora (the plural of corpus) may be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'alpino', 'alpino.zip', 'biocreative_ppi', 'biocreative_ppi.zip', 'brown', 'brown.zip', 'brown_tei', 'brown_tei.zip', 'cess_cat', 'cess_cat.zip', 'cess_esp', 'cess_esp.zip', 'chat80', 'chat80.zip', 'city_database', 'city_database.zip', 'cmudict', 'cmudict.zip', 'comparative_sentences', 'comparative_sentences.zip', 'comtrans.zip', 'conll2000', 'conll2000.zip', 'conll2002', 'conll2002.zip', 'conll2007.zip', 'crubadan', 'crubadan.zip', 'dependency_treebank', 'dependency_treebank.zip', 'dolch', 'dolch.zip', 'europarl_raw', 'europarl_raw.zip', 'floresta', 'floresta.zip', 'framenet_v15', 'framenet_v15.zip', 'framenet_v17.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'ieer', 'ieer.zip', 'inaugural', 'inaugural.zip', 'indian', 'indian.zip', 'jeita.zip', 'kimmo', 'kimmo.zip', 'knbc.zip', 'lin_thesaurus', 'lin_thesaurus.zip', 'machado.zip', 'mac_morpho', 'mac_morpho.zip', 'masc_tagged.zip', 'movie_reviews', 'movie_reviews.zip', 'mte_teip5', 'mte_teip5.zip', 'names', 'names.zip', 'nombank.1.0.zip', 'nonbreaking_prefixes', 'nonbreaking_prefixes.zip', 'nps_chat', 'nps_chat.zip', 'omw', 'omw.zip', 'opinion_lexicon', 'opinion_lexicon.zip', 'panlex_swadesh.zip', 'paradigms', 'paradigms.zip', 'pil', 'pil.zip', 'pl196x', 'pl196x.zip', 'ppattach', 'ppattach.zip', 'problem_reports', 'problem_reports.zip', 'product_reviews_1', 'product_reviews_1.zip', 'product_reviews_2', 'product_reviews_2.zip', 'propbank.zip', 'pros_cons', 'pros_cons.zip', 'ptb', 'ptb.zip', 'qc', 'qc.zip', 'reuters.zip', 'rte', 'rte.zip', 'semcor.zip', 'senseval', 'senseval.zip', 'sentence_polarity', 'sentence_polarity.zip', 'sentiwordnet', 'sentiwordnet.zip', 'shakespeare', 'shakespeare.zip', 'sinica_treebank', 'sinica_treebank.zip', 'smultron', 'smultron.zip', 'state_union', 'state_union.zip', 'stopwords', 'stopwords.zip', 'subjectivity', 'subjectivity.zip', 'swadesh', 'swadesh.zip', 'switchboard', 'switchboard.zip', 'timit', 'timit.zip', 'toolbox', 'toolbox.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'udhr', 'udhr.zip', 'udhr2', 'udhr2.zip', 'unicode_samples', 'unicode_samples.zip', 'universal_treebanks_v20.zip', 'verbnet', 'verbnet.zip', 'verbnet3', 'verbnet3.zip', 'webtext', 'webtext.zip', 'wordnet', 'wordnet.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip', 'ycoe', 'ycoe.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find('corpora')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words() # all these files are present in brown ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids() # so under gutenberg file ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Julius', 'Caesar', ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet=nltk.corpus.gutenberg.words('shakespeare-caesar.txt')\n",
    "hamlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if want to see the 500 words of text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The Tragedie of Julius Caesar by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Flauius , Murellus , and certaine Commoners ouer the Stage . Flauius . Hence : home you idle Creatures , get you home : Is this a Holiday ? What , know you not ( Being Mechanicall ) you ought not walke Vpon a labouring day , without the signe Of your Profession ? Speake , what Trade art thou ? Car . Why Sir , a Carpenter Mur . Where is thy Leather Apron , and thy Rule ? What dost thou with thy best Apparrell on ? You sir , what Trade are you ? Cobl . Truely Sir , in respect of a fine Workman , I am but as you would say , a Cobler Mur . But what Trade art thou ? Answer me directly Cob . A Trade Sir , that I hope I may vse , with a safe Conscience , which is indeed Sir , a Mender of bad soules Fla . What Trade thou knaue ? Thou naughty knaue , what Trade ? Cobl . Nay I beseech you Sir , be not out with me : yet if you be out Sir , I can mend you Mur . What mean ' st thou by that ? Mend mee , thou sawcy Fellow ? Cob . Why sir , Cobble you Fla . Thou art a Cobler , art thou ? Cob . Truly sir , all that I liue by , is with the Aule : I meddle with no Tradesmans matters , nor womens matters ; but withal I am indeed Sir , a Surgeon to old shooes : when they are in great danger , I recouer them . As proper men as euer trod vpon Neats Leather , haue gone vpon my handy - worke Fla . But wherefore art not in thy Shop to day ? Why do ' st thou leade these men about the streets ? Cob . Truly sir , to weare out their shooes , to get my selfe into more worke . But indeede sir , we make Holyday to see Caesar , and to reioyce in his Triumph Mur . Wherefore reioyce ? What Conquest brings he home ? What Tributaries follow him to Rome , To grace in Captiue bonds his Chariot Wheeles ? You Blockes , you stones , you worse then senslesse things : O you hard hearts , you cruell men of Rome , Knew you not Pompey many a time and oft ? Haue you climb ' d vp to Walles and Battlements , To Towres and Windowes ? Yea , to Chimney tops , Your Infants in your Armes , and there haue sate The liue - long day , with patient expectation , To see great Pompey passe the streets of Rome : And when you saw his Chariot but appeare , Haue you "
     ]
    }
   ],
   "source": [
    "for word in hamlet[:500]:\n",
    "    print(word,sep=' ',end=' ') #sep parameter in print() The separator between the arguments to print() function in Python is space by default (softspace feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our own words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=('''Natural language processing (NLP) is a subfield of linguistics,computer science, and artificial intelligence concerned with the interactions between computers and human language, \n",
    "        in particular how to program computers to process and analyze large amounts of natural language data. \n",
    "\n",
    "        The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. \n",
    "        1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, \n",
    "        a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.''')\n",
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'subfield',\n",
       " 'of',\n",
       " 'linguistics',\n",
       " ',',\n",
       " 'computer',\n",
       " 'science',\n",
       " ',',\n",
       " 'and',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'concerned',\n",
       " 'with',\n",
       " 'the',\n",
       " 'interactions',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " 'language',\n",
       " ',',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'how',\n",
       " 'to',\n",
       " 'program',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'process',\n",
       " 'and',\n",
       " 'analyze',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data',\n",
       " '.',\n",
       " 'The',\n",
       " 'result',\n",
       " 'is',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'capable',\n",
       " 'of',\n",
       " '``',\n",
       " 'understanding',\n",
       " \"''\",\n",
       " 'the',\n",
       " 'contents',\n",
       " 'of',\n",
       " 'documents',\n",
       " ',',\n",
       " 'including',\n",
       " 'the',\n",
       " 'contextual',\n",
       " 'nuances',\n",
       " 'of',\n",
       " 'the',\n",
       " 'language',\n",
       " 'within',\n",
       " 'them',\n",
       " '.',\n",
       " 'The',\n",
       " 'technology',\n",
       " 'can',\n",
       " 'then',\n",
       " 'accurately',\n",
       " 'extract',\n",
       " 'information',\n",
       " 'and',\n",
       " 'insights',\n",
       " 'contained',\n",
       " 'in',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'categorize',\n",
       " 'and',\n",
       " 'organize',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'themselves',\n",
       " '.',\n",
       " '1960s',\n",
       " ':',\n",
       " 'Some',\n",
       " 'notably',\n",
       " 'successful',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'systems',\n",
       " 'developed',\n",
       " 'in',\n",
       " 'the',\n",
       " '1960s',\n",
       " 'were',\n",
       " 'SHRDLU',\n",
       " ',',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'system',\n",
       " 'working',\n",
       " 'in',\n",
       " 'restricted',\n",
       " '``',\n",
       " 'blocks',\n",
       " 'worlds',\n",
       " \"''\",\n",
       " 'with',\n",
       " 'restricted',\n",
       " 'vocabularies',\n",
       " ',',\n",
       " 'and',\n",
       " 'ELIZA',\n",
       " ',',\n",
       " 'a',\n",
       " 'simulation',\n",
       " 'of',\n",
       " 'a',\n",
       " 'Rogerian',\n",
       " 'psychotherapist',\n",
       " ',',\n",
       " 'written',\n",
       " 'by',\n",
       " 'Joseph',\n",
       " 'Weizenbaum',\n",
       " 'between',\n",
       " '1964',\n",
       " 'and',\n",
       " '1966',\n",
       " '.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_token=word_tokenize(nlp)\n",
    "nlp_token  # divide whole paragraph into tokens ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp_token) #now we have to look no. of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist #NLTK in python has a function FreqDist which gives you the frequency of words within a text.\n",
    "fdist=FreqDist()\n",
    "# Now from nltk we have prob. function which is FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 9, ',': 8, 'and': 7, 'language': 6, 'of': 6, 'a': 5, 'natural': 4, 'in': 4, '.': 4, 'documents': 3, ...})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in nlp_token:\n",
    "    fdist[word.lower()]+=1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so from given output paragraph mei the 8 bar h, of 5 bar h and so on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist['the'] #the 8 times h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'t': 1, 'h': 1, 'e': 1})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdist)  # distinct token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 9),\n",
       " (',', 8),\n",
       " ('and', 7),\n",
       " ('language', 6),\n",
       " ('of', 6),\n",
       " ('a', 5),\n",
       " ('natural', 4),\n",
       " ('in', 4),\n",
       " ('.', 4),\n",
       " ('documents', 3)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10=fdist.most_common(10)\n",
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "nlp_blank=blankline_tokenize(nlp)\n",
    "len(nlp_blank)\n",
    "#no. of paragraph which are seperated by new lines in our given document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'processing'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_token[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. \\n        1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, \\n        a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_blank[1] # if want to see second paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tokenization parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Bigrams:- Tokens of two consecutive written words known as bigrams.\n",
    "\n",
    "2)Trigrams:-Token of three..\n",
    "\n",
    "3)Ngrams:- Token of any.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data', 'is', 'the', 'new', 'science']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string='Data is the new science'\n",
    "quotes_tokens=nltk.word_tokenize(string)\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'is'), ('is', 'the'), ('the', 'new'), ('new', 'science')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_bigrams=list(nltk.bigrams(quotes_tokens))\n",
    "quotes_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'is', 'the'), ('is', 'the', 'new'), ('the', 'new', 'science')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_trigrams=list(nltk.trigrams(quotes_tokens))\n",
    "quotes_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'is', 'the', 'new', 'science')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_ngrams=list(nltk.ngrams(quotes_tokens,5))\n",
    "quotes_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize words into its base form or root form\n",
    "\n",
    "Ex:- affection, affects, affections, affected, affecting\n",
    "\n",
    "all of these words are originated from single root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer #'Porter stemmer') is a process for removing the commoner morphological and inflexional endings from words in English.\n",
    "pst=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feel'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('feeling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "given:given\n",
      "giving:give\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "words_to_stem=['give','given','giving','gave']\n",
    "for words in words_to_stem:\n",
    "    print(words+':'+pst.stem(words))\n",
    "# so stemmer remove only ing and replace with e ðŸ‘‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LancasterStemmer is just for fun, the Lancaster stemming algorithm is another algorithm that you can use. \n",
    "This one is the most aggressive stemming algorithm of the bunch. However, if you use the stemmer in NLTK, \n",
    "you can add your own custom rules to this algorithm very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:giv\n",
      "given:giv\n",
      "giving:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst=LancasterStemmer()\n",
    "for words in words_to_stem:\n",
    "    print(words+':'+lst.stem(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowball Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowball Stemmer is the stemming alg. which is also known as the porter2 stemming alg as it is a better version of the PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbst=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "given:given\n",
      "giving:give\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words+':'+sbst.stem(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WordNet is a part of Python's Natural Language Toolkit. It is a large word database of English Nouns, Adjectives, Adverbs and Verbs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lem.lemmatize('corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "given:given\n",
      "giving:giving\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem:\n",
    "    print(words+':'+word_lem.lemmatize(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in nlp useless words(data) are referred to as stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 9),\n",
       " (',', 8),\n",
       " ('and', 7),\n",
       " ('language', 6),\n",
       " ('of', 6),\n",
       " ('a', 5),\n",
       " ('natural', 4),\n",
       " ('in', 4),\n",
       " ('.', 4),\n",
       " ('documents', 3)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "puntuation=re.compile(r'[-,?,!]') #remove kr dega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_puntuation=[]\n",
    "for words in nlp_token:\n",
    "    word=puntuation.sub('',words)\n",
    "    if len(word)>0:\n",
    "        post_puntuation.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'subfield',\n",
       " 'of',\n",
       " 'linguistics',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'and',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'concerned',\n",
       " 'with',\n",
       " 'the',\n",
       " 'interactions',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " 'language',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'how',\n",
       " 'to',\n",
       " 'program',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'process',\n",
       " 'and',\n",
       " 'analyze',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data',\n",
       " '.',\n",
       " 'The',\n",
       " 'result',\n",
       " 'is',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'capable',\n",
       " 'of',\n",
       " '``',\n",
       " 'understanding',\n",
       " \"''\",\n",
       " 'the',\n",
       " 'contents',\n",
       " 'of',\n",
       " 'documents',\n",
       " 'including',\n",
       " 'the',\n",
       " 'contextual',\n",
       " 'nuances',\n",
       " 'of',\n",
       " 'the',\n",
       " 'language',\n",
       " 'within',\n",
       " 'them',\n",
       " '.',\n",
       " 'The',\n",
       " 'technology',\n",
       " 'can',\n",
       " 'then',\n",
       " 'accurately',\n",
       " 'extract',\n",
       " 'information',\n",
       " 'and',\n",
       " 'insights',\n",
       " 'contained',\n",
       " 'in',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'categorize',\n",
       " 'and',\n",
       " 'organize',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'themselves',\n",
       " '.',\n",
       " '1960s',\n",
       " ':',\n",
       " 'Some',\n",
       " 'notably',\n",
       " 'successful',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'systems',\n",
       " 'developed',\n",
       " 'in',\n",
       " 'the',\n",
       " '1960s',\n",
       " 'were',\n",
       " 'SHRDLU',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'system',\n",
       " 'working',\n",
       " 'in',\n",
       " 'restricted',\n",
       " '``',\n",
       " 'blocks',\n",
       " 'worlds',\n",
       " \"''\",\n",
       " 'with',\n",
       " 'restricted',\n",
       " 'vocabularies',\n",
       " 'and',\n",
       " 'ELIZA',\n",
       " 'a',\n",
       " 'simulation',\n",
       " 'of',\n",
       " 'a',\n",
       " 'Rogerian',\n",
       " 'psychotherapist',\n",
       " 'written',\n",
       " 'by',\n",
       " 'Joseph',\n",
       " 'Weizenbaum',\n",
       " 'between',\n",
       " '1964',\n",
       " 'and',\n",
       " '1966',\n",
       " '.']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_puntuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent='Bhagat Singh was an Indian socialist revolutionary whose two acts of dramatic violence against the British in India and execution at age 23 made him a folk hero of the Indian independence movement.'\n",
    "sent_tokens=word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bhagat', 'NN')]\n",
      "[('Singh', 'NN')]\n",
      "[('was', 'VBD')]\n",
      "[('an', 'DT')]\n",
      "[('Indian', 'JJ')]\n",
      "[('socialist', 'NN')]\n",
      "[('revolutionary', 'JJ')]\n",
      "[('whose', 'WP$')]\n",
      "[('two', 'CD')]\n",
      "[('acts', 'NNS')]\n",
      "[('of', 'IN')]\n",
      "[('dramatic', 'JJ')]\n",
      "[('violence', 'NN')]\n",
      "[('against', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('British', 'JJ')]\n",
      "[('in', 'IN')]\n",
      "[('India', 'NNP')]\n",
      "[('and', 'CC')]\n",
      "[('execution', 'NN')]\n",
      "[('at', 'IN')]\n",
      "[('age', 'NN')]\n",
      "[('23', 'CD')]\n",
      "[('made', 'VBN')]\n",
      "[('him', 'PRP')]\n",
      "[('a', 'DT')]\n",
      "[('folk', 'NN')]\n",
      "[('hero', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('Indian', 'JJ')]\n",
      "[('independence', 'NN')]\n",
      "[('movement', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for token in sent_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ne_chunk:-NLTK provides a classifier that has already been trained to recognize named entities, accessed with the function nltk.ne_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_sent='Vinayak Damodar Savarkar, commonly known as Swatantryaveer Savarkar or simply Veer Savarkar in Marathi language, was an Indian independence activist and politician who formulated the Hindu nationalist philosophy of Hindutva. He was a leading figure in the Hindu Mahasabha'\n",
    "ne_tokens=word_tokenize(ne_sent) #tokenize this sentences\n",
    "ne_tags=nltk.pos_tag(ne_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Vinayak/NNP)\n",
      "  (PERSON Damodar/NNP Savarkar/NNP)\n",
      "  ,/,\n",
      "  commonly/RB\n",
      "  known/VBN\n",
      "  as/IN\n",
      "  (PERSON Swatantryaveer/NNP Savarkar/NNP)\n",
      "  or/CC\n",
      "  simply/RB\n",
      "  (PERSON Veer/NNP Savarkar/NNP)\n",
      "  in/IN\n",
      "  (GPE Marathi/NNP)\n",
      "  language/NN\n",
      "  ,/,\n",
      "  was/VBD\n",
      "  an/DT\n",
      "  (GPE Indian/JJ)\n",
      "  independence/NN\n",
      "  activist/NN\n",
      "  and/CC\n",
      "  politician/NN\n",
      "  who/WP\n",
      "  formulated/VBD\n",
      "  the/DT\n",
      "  (ORGANIZATION Hindu/NNP)\n",
      "  nationalist/JJ\n",
      "  philosophy/NN\n",
      "  of/IN\n",
      "  (GPE Hindutva/NNP)\n",
      "  ./.\n",
      "  He/PRP\n",
      "  was/VBD\n",
      "  a/DT\n",
      "  leading/JJ\n",
      "  figure/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Hindu/NNP Mahasabha/NNP))\n"
     ]
    }
   ],
   "source": [
    "ne_ner=ne_chunk(ne_tags)\n",
    "print(ne_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking up individual pieces of information and grouping them into bigger pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Shivaram', 'NNP'),\n",
       " ('Hari', 'NNP'),\n",
       " ('Rajguru', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('an', 'DT'),\n",
       " ('Indian', 'JJ'),\n",
       " ('revolutionary', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('Maharashtra', 'NNP'),\n",
       " (',', ','),\n",
       " ('known', 'VBN'),\n",
       " ('mainly', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('involvement', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('assassination', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('British', 'JJ'),\n",
       " ('Raj', 'NNP'),\n",
       " ('police', 'NNS'),\n",
       " ('officer', 'NN'),\n",
       " ('.', '.'),\n",
       " ('He', 'PRP'),\n",
       " ('also', 'RB'),\n",
       " ('fought', 'VBD'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('independence', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('India', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('On', 'IN'),\n",
       " ('23', 'CD'),\n",
       " ('March', 'NNP'),\n",
       " ('1931', 'CD'),\n",
       " ('he', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('hanged', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('British', 'JJ'),\n",
       " ('government', 'NN'),\n",
       " ('along', 'IN'),\n",
       " ('with', 'IN'),\n",
       " ('Bhagat', 'NNP'),\n",
       " ('Singh', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Sukhdev', 'NNP'),\n",
       " ('Thapar', 'NNP')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new='Shivaram Hari Rajguru was an Indian revolutionary from Maharashtra, known mainly for his involvement in the assassination of a British Raj police officer. He also fought for the independence of India and On 23 March 1931 he was hanged by the British government along with Bhagat Singh and Sukhdev Thapar'\n",
    "new_tokens=nltk.pos_tag(word_tokenize(new)) #tokenize kr rhe h or sath mei pos tag bhi add kr rhe h\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer_np=r'NP:{<DT>?<JJ>*<NN>}' #grammer ko define kr rhe h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_parser=nltk.RegexpParser(grammer_np)\n",
    "# create a regular expression parser for (grammer_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Ghostscript executable isn't found.\n",
      "See http://web.mit.edu/ghostscript/www/Install.htm\n",
      "If you're using a Mac, you can try installing\n",
      "https://docs.brew.sh/Installation then `brew install ghostscript`\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    797\u001b[0m                     [\n\u001b[1;32m--> 798\u001b[1;33m                         find_binary(\n\u001b[0m\u001b[0;32m    799\u001b[0m                             \u001b[1;34m\"gs\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    687\u001b[0m ):\n\u001b[1;32m--> 688\u001b[1;33m     return next(\n\u001b[0m\u001b[0;32m    689\u001b[0m         find_binary_iter(\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \"\"\"\n\u001b[1;32m--> 673\u001b[1;33m     for file in find_file_iter(\n\u001b[0m\u001b[0;32m    674\u001b[0m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"=\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\n%s\\n%s\\n%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    815\u001b[0m                 )\n\u001b[0;32m    816\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_error_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 817\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [('Shivaram', 'NNP'), ('Hari', 'NNP'), ('Rajguru', 'NNP'), ('was', 'VBD'), Tree('NP', [('an', 'DT'), ('Indian', 'JJ'), ('revolutionary', 'NN')]), ('from', 'IN'), ('Maharashtra', 'NNP'), (',', ','), ('known', 'VBN'), ('mainly', 'RB'), ('for', 'IN'), ('his', 'PRP$'), Tree('NP', [('involvement', 'NN')]), ('in', 'IN'), Tree('NP', [('the', 'DT'), ('assassination', 'NN')]), ('of', 'IN'), ('a', 'DT'), ('British', 'JJ'), ('Raj', 'NNP'), ('police', 'NNS'), Tree('NP', [('officer', 'NN')]), ('.', '.'), ('He', 'PRP'), ('also', 'RB'), ('fought', 'VBD'), ('for', 'IN'), Tree('NP', [('the', 'DT'), ('independence', 'NN')]), ('of', 'IN'), ('India', 'NNP'), ('and', 'CC'), ('On', 'IN'), ('23', 'CD'), ('March', 'NNP'), ('1931', 'CD'), ('he', 'PRP'), ('was', 'VBD'), ('hanged', 'VBN'), ('by', 'IN'), Tree('NP', [('the', 'DT'), ('British', 'JJ'), ('government', 'NN')]), ('along', 'IN'), ('with', 'IN'), ('Bhagat', 'NNP'), ('Singh', 'NNP'), ('and', 'CC'), ('Sukhdev', 'NNP'), ('Thapar', 'NNP')])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_result=chunk_parser.parse(new_tokens)\n",
    "chunk_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
